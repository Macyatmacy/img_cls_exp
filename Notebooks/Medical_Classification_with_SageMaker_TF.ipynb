{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Training, Deployment & Prediction Using the AWS SageMaker and Tensorflow\n",
    "\n",
    "## An experiment on medical images\n",
    "\n",
    "1. [Import Requirements and Set Up](#Import-Requirements-and-Set-Up)\n",
    "1. [Prepare the Dataset](#Prepare-the-Dataset)\n",
    "1. [Prepare Training Script](#Prepare-Training-Script)\n",
    "1. [Train with TensorFlow Estimator](#Train-with-TensorFlow-Estimator)\n",
    "1. [Deploy TensorFlow Model](#Deploy-TensorFlow-Model)\n",
    "1. [Test and Prediction Task](#Test-and-Prediction-Task)\n",
    "1. [Delete the Endpoint](#Delete-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Requirements and Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Modules\n",
    "First, we'll need to load all the required modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import sagemaker \n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "session = sagemaker.Session() \n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "This notebook uses a pretrained mobilenet model to save training time.\n",
    "\n",
    "We use AWS S3 to store and manage our image data. In this notebook, we only use 1000 normal images and 1000 pneumonia images from the ChestXray2017 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"pn_deploy\"\n",
    "\n",
    "train_prefix = \"train\"\n",
    "val_prefix = \"validation\"\n",
    "\n",
    "train_data = \"s3://{}/{}/{}/\".format(bucket, project_name, train_prefix)\n",
    "validation_data = \"s3://{}/{}/{}/\".format(bucket, project_name, val_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download or Update Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(bucket_name, prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket=bucket_name\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    location_list = []\n",
    "    for (bucket_name, key) in map(lambda x: (x.bucket_name, x.key), my_bucket.objects.filter(Prefix=prefix)):\n",
    "        data_location = \"s3://{}/{}\".format(bucket_name, key)\n",
    "        location_list.append(data_location)\n",
    "    # Remove the root folder path\n",
    "    if \"s3://{}/{}/\".format(bucket_name, prefix) in location_list:\n",
    "        location_list.remove(\"s3://{}/{}/\".format(bucket_name, prefix))\n",
    "    return location_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_normal = get_file_list(bucket,\"pn_deploy/normal_1000\")\n",
    "list_pneumonia = get_file_list(bucket,\"pn_deploy/pneumonia_1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "for l in list_normal:\n",
    "    data_source1 = S3Downloader.download(\n",
    "    local_path=\"/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/normal/\",\n",
    "    s3_uri=l,\n",
    "    sagemaker_session=session,\n",
    "    )\n",
    "\n",
    "for l in list_pneumonia:\n",
    "    data_source1 = S3Downloader.download(\n",
    "    local_path=\"/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/pneumonia/\",\n",
    "    s3_uri=l,\n",
    "    sagemaker_session=session,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate annotations\n",
    "import os\n",
    "os.mkdir('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/annotations')\n",
    "\n",
    "filePath = '/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/normal'\n",
    "l = os.listdir(filePath)\n",
    "ant={}\n",
    "for n in l:\n",
    "    name,_ = n.split('.')\n",
    "    ant[name] = 'normal'\n",
    "with open('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/annotations/normal.txt', 'w') as f:\n",
    "    for n, c in ant.items():\n",
    "        f.write(str(n)+\" \"+str(c)+\"\\n\")\n",
    "\n",
    "filePath = '/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/pneumonia'\n",
    "l = os.listdir(filePath)\n",
    "ant={}\n",
    "for n in l:\n",
    "    name,_ = n.split('.')\n",
    "    ant[name] = 'pneumonia'\n",
    "with open('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/annotations/pneumonia.txt', 'w') as f:\n",
    "    for n, c in ant.items():\n",
    "        f.write(str(n)+\" \"+str(c)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotations\n",
    "def get_annotations(file_path, annotations={}):\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        rows = f.read().splitlines()\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        image_name, class_name = row.split(' ')\n",
    "        image_name = image_name + '.jpeg'\n",
    "        \n",
    "        annotations[image_name] = class_name\n",
    "    \n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total normal examples 1000\n",
      "Total pneumonia examples 1000\n"
     ]
    }
   ],
   "source": [
    "# read annotations\n",
    "annotations_normal={}\n",
    "annotations_pneumonia={}\n",
    "annotations_normal = get_annotations('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/annotations/normal.txt',\n",
    "                                     annotations_normal)\n",
    "annotations_pneumonia = get_annotations('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/annotations/pneumonia.txt',\n",
    "                                        annotations_pneumonia)\n",
    "\n",
    "total_count = len(annotations_normal.keys())\n",
    "print('Total normal examples', total_count)\n",
    "total_count = len(annotations_pneumonia.keys())\n",
    "print('Total pneumonia examples', total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NORMAL2-IM-0774-0001.jpeg', 'normal')\n",
      "('person109_virus_203.jpeg', 'pneumonia')\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(annotations_normal.items())))\n",
    "print(next(iter(annotations_pneumonia.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data and Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and copy file\n",
    "import os\n",
    "classes = ['normal', 'pneumonia']\n",
    "sets = ['train', 'validation']\n",
    "root_dir = '/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/custom_data'\n",
    "\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "    \n",
    "for set_name in sets:\n",
    "    if not os.path.isdir(os.path.join(root_dir, set_name)):\n",
    "        os.mkdir(os.path.join(root_dir, set_name))\n",
    "    for class_name in classes:\n",
    "        folder = os.path.join(root_dir, set_name, class_name)\n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, class_name in annotations_normal.items():\n",
    "    target_set = 'validation' if random.randint(0, 99) < 20 else 'train'\n",
    "    target_path = os.path.join(root_dir, target_set, class_name, image)\n",
    "    shutil.copy(os.path.join('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/normal', image), target_path)\n",
    "\n",
    "for image, class_name in annotations_pneumonia.items():\n",
    "    target_set = 'validation' if random.randint(0, 99) < 20 else 'train'\n",
    "    target_path = os.path.join(root_dir, target_set, class_name, image)\n",
    "    shutil.copy(os.path.join('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/data/pneumonia', image), target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/img_cls_exp/Medical Image/Pneumonia/custom_data/train/normal has 814 images\n",
      "/home/ec2-user/SageMaker/img_cls_exp/Medical Image/Pneumonia/custom_data/train/pneumonia has 790 images\n",
      "/home/ec2-user/SageMaker/img_cls_exp/Medical Image/Pneumonia/custom_data/validation/normal has 186 images\n",
      "/home/ec2-user/SageMaker/img_cls_exp/Medical Image/Pneumonia/custom_data/validation/pneumonia has 210 images\n",
      "{'train': 1604, 'validation': 396}\n"
     ]
    }
   ],
   "source": [
    "sets_counts = {\n",
    "    'train': 0,\n",
    "    'validation': 0\n",
    "}\n",
    "\n",
    "for set_name in sets:\n",
    "    for class_name in classes:\n",
    "        path = os.path.join(root_dir, set_name, class_name)\n",
    "        count = len(os.listdir(path))\n",
    "        print(path, 'has', count, 'images')\n",
    "        sets_counts[set_name] += count\n",
    "\n",
    "print(sets_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to S3..\n",
      "Uploaded to s3://sagemaker-us-east-2-179199196742/pn_deploy\n"
     ]
    }
   ],
   "source": [
    "print('Uploading to S3..')\n",
    "s3_data_path = session.upload_data(path=root_dir, bucket=bucket, key_prefix='pn_deploy')\n",
    "\n",
    "print('Uploaded to', s3_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet',\n",
    "                                                       pooling='avg', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.layers[0].trainable = True\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "def create_data_generators(root_dir, batch_size):\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=[0.8, 1.2],\n",
    "        rotation_range=20\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'train'),\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'validation'),\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    return train_data_generator, val_data_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--steps', type=int, default=int(1614/16))\n",
    "    parser.add_argument('--val_steps', type=int, default=int(386/16))\n",
    "\n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model-dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    local_output_dir = args.sm_model_dir\n",
    "    local_root_dir = args.train\n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "    model = create_model()\n",
    "    train_gen, val_gen = create_data_generators(local_root_dir, batch_size)\n",
    "    \n",
    "    _ = model.fit(\n",
    "        train_gen,\n",
    "        epochs=args.epochs,\n",
    "        steps_per_epoch=args.steps,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=args.val_steps\n",
    "    )\n",
    "    \n",
    "    model.save(os.path.join(local_output_dir, 'model', '1'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with TensorFlow Estimator\n",
    "You can check [SageMaker endpoints and quotas](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html#limits_sagemaker) for sagemaker training and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.m5.4xlarge', # you can use any instance_type within your quotas\n",
    "    framework_version='2.1.0',\n",
    "    py_version='py3',\n",
    "    output_path=\"s3://{}/{}\".format(bucket, project_name),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_data_path = 's3://sagemaker-us-east-2-179199196742/pn_deploy'\n",
    "estimator.fit(s3_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!\n",
      "Model Deployed!\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "print('\\nModel Deployed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/img_cls_exp/Medical_Image/Pneumonia/test/NORMAL2-IM-1436-0001.jpeg'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "list_normal_test = get_file_list(bucket, \"pn_deploy/test/normal\")\n",
    "list_pneumonia_test = get_file_list(bucket, \"pn_deploy/test/pneumonia\")\n",
    "if not os.path.isdir('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/test/'):\n",
    "    os.mkdir('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/test/')\n",
    "test = list_normal_test[0]\n",
    "data_source = S3Downloader.download(local_path='/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/test/', s3_uri=test)\n",
    "image_path = '/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/test/' + test[60:]\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def get_pred(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    results = predictor.predict(img)\n",
    "    class_id = int(np.squeeze(results['predictions']) > 0.5)\n",
    "    return classes[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normal'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Task\n",
    "We create a folder in S3 bucket to store data to be predict, running the following cells will make predictions and send the output back to the same folder in S3 bucket and Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task link and list\n",
    "list_task = get_file_list(bucket, \"pn_deploy/task/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list_task:\n",
    "    data_source = S3Downloader.download(\n",
    "    local_path='/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/task/data/',\n",
    "    s3_uri=l,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = []\n",
    "for l in list_task:\n",
    "    image_path.append('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/task/' + l[53:])\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/task/prediction_output.txt', 'w') as f:\n",
    "    for i in image_path:\n",
    "        f.write(i[70:] + \" \" + get_pred(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to S3..\n",
      "Uploaded to s3://sagemaker-us-east-2-179199196742/pn_deploy/task/pred_output/prediction_output.txt\n"
     ]
    }
   ],
   "source": [
    "print('Uploading to S3..')\n",
    "s3_data_path = session.upload_data(path='/home/ec2-user/SageMaker/img_cls_exp/MedicalImage/Pneumonia/task/prediction_output.txt', bucket=bucket, key_prefix='pn_deploy/task/pred_output')\n",
    "print('Uploaded to', s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "When you are done, make sure to clean up your AWS account by deleting resources you won't be reusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
